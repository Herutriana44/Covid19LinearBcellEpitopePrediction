{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset Management - Combined Version using PySpark\n",
        "\n",
        "This notebook combines manage1.ipynb and manage1(vers2).ipynb functionality using PySpark for efficient data processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize PySpark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import pandas as pd\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"EpitopeDatasetManagement\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "print(\"Spark session created successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Load and Preprocess Data (from manage1.ipynb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load positive and negative datasets\n",
        "pos_pd = pd.read_csv('positif.csv')\n",
        "neg_pd = pd.read_excel('negatif.xlsx')\n",
        "\n",
        "# Convert to Spark DataFrames\n",
        "pos_df = spark.createDataFrame(pos_pd)\n",
        "neg_df = spark.createDataFrame(neg_pd)\n",
        "\n",
        "print(f\"Positive samples: {pos_df.count()}\")\n",
        "print(f\"Negative samples: {neg_df.count()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add labels and actual columns\n",
        "pos_df = pos_df.withColumn('label', lit('E')) \\\n",
        "    .withColumn('actual', expr(\"repeat('E', length(`Epitope - Name`))\"))\n",
        "\n",
        "neg_df = neg_df.withColumn('label', lit('.')) \\\n",
        "    .withColumn('actual', expr(\"repeat('.', length(`Epitope - Name`))\"))\n",
        "\n",
        "# Select required columns\n",
        "sel_col = ['Epitope - Starting Position', 'Epitope - Ending Position', 'Epitope - Name', 'label', 'actual']\n",
        "pos_df = pos_df.select(sel_col)\n",
        "neg_df = neg_df.select(sel_col)\n",
        "\n",
        "# Combine datasets\n",
        "df1_merge = pos_df.union(neg_df)\n",
        "print(f\"Combined dataset size: {df1_merge.count()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define protein preprocessing function\n",
        "def protein_preprocessing(sequence):\n",
        "    \"\"\"Filter valid amino acid characters\"\"\"\n",
        "    amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
        "    processed = ''.join([char for char in sequence if char.upper() in amino_acids])\n",
        "    return processed\n",
        "\n",
        "# Register UDF\n",
        "from pyspark.sql.functions import udf\n",
        "preprocess_udf = udf(protein_preprocessing, StringType())\n",
        "\n",
        "# Apply preprocessing\n",
        "df1_merge = df1_merge.withColumn('Epitope - Name', preprocess_udf(col('Epitope - Name')))\n",
        "\n",
        "# Drop null values\n",
        "df1_merge = df1_merge.filter(\n",
        "    col('Epitope - Starting Position').isNotNull() & \n",
        "    col('Epitope - Ending Position').isNotNull()\n",
        ")\n",
        "\n",
        "print(f\"After preprocessing: {df1_merge.count()} rows\")\n",
        "df1_merge.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save dataset_type_1.csv\n",
        "df1_merge_pd = df1_merge.toPandas()\n",
        "df1_merge_pd.to_csv('dataset_type_1.csv', index=False)\n",
        "print(\"Saved dataset_type_1.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define function to create position sequence\n",
        "def create_position_sequence(start, end):\n",
        "    \"\"\"Create list of positions from start to end\"\"\"\n",
        "    return [float(start + i) for i in range(int(end - start + 1))]\n",
        "\n",
        "# Register UDF\n",
        "from pyspark.sql.functions import explode, arrays_zip, split, lit, col\n",
        "\n",
        "position_sequence_udf = udf(create_position_sequence, ArrayType(DoubleType()))\n",
        "\n",
        "# Create arrays for amino acids, labels, and positions\n",
        "df1_merge = df1_merge.withColumn(\n",
        "    'amino_array',\n",
        "    split(col('Epitope - Name'), '')\n",
        ").withColumn(\n",
        "    'label_array',\n",
        "    split(col('actual'), '')\n",
        ").withColumn(\n",
        "    'position_array',\n",
        "    position_sequence_udf(\n",
        "        col('Epitope - Starting Position'),\n",
        "        col('Epitope - Ending Position')\n",
        "    )\n",
        ")\n",
        "\n",
        "# Explode arrays to create individual rows\n",
        "df2_merge = df1_merge.select(\n",
        "    explode(arrays_zip(\n",
        "        col('amino_array'),\n",
        "        col('label_array'),\n",
        "        col('position_array')\n",
        "    )).alias('zipped')\n",
        ").select(\n",
        "    col('zipped.0').alias('amino'),\n",
        "    col('zipped.1').alias('label'),\n",
        "    col('zipped.2').alias('Position')\n",
        ")\n",
        "\n",
        "# Filter out empty amino acids\n",
        "df2_merge = df2_merge.filter(col('amino') != '')\n",
        "\n",
        "print(f\"Dataset type 2 size: {df2_merge.count()}\")\n",
        "df2_merge.show(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Add Hydrophobicity Scales (from manage1(vers2).ipynb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define hydrophobicity scales\n",
        "def kyte_doolittle_scale(aa):\n",
        "    scale = {'A': 1.8, 'C': 2.5, 'D': -3.5, 'E': -3.5, 'F': 2.8, 'G': -0.4, 'H': -3.2, 'I': 4.5, 'K': -3.9, 'L': 3.8, 'M': 1.9, 'N': -3.5, 'P': -1.6, 'Q': -3.5, 'R': -4.5, 'S': -0.8, 'T': -0.7, 'V': 4.2, 'W': -0.9, 'Y': -1.3}\n",
        "    return scale.get(aa, 0.0)\n",
        "\n",
        "def hopp_woods_scale(aa):\n",
        "    scale = {'A': -0.5, 'C': -1.0, 'D': 3.0, 'E': 3.0, 'F': -2.5, 'G': 0.0, 'H': -0.5, 'I': -1.8, 'K': 3.0, 'L': -1.8, 'M': -1.3, 'N': 0.2, 'P': 0.0, 'Q': 0.2, 'R': 3.0, 'S': 0.3, 'T': -0.4, 'V': -1.5, 'W': -3.4, 'Y': -2.3}\n",
        "    return scale.get(aa, 0.0)\n",
        "\n",
        "def cornette_scale(aa):\n",
        "    scale = {'A': 0.2, 'C': 4.1, 'D': -3.1, 'E': -1.8, 'F': 4.4, 'G': 0.0, 'H': 0.5, 'I': 4.8, 'K': -3.1, 'L': 5.7, 'M': 4.2, 'N': -0.5, 'P': -2.2, 'Q': -2.8, 'R': 1.4, 'S': -0.5, 'T': -1.9, 'V': 4.7, 'W': 1.0, 'Y': 3.2}\n",
        "    return scale.get(aa, 0.0)\n",
        "\n",
        "def eisenberg_scale(aa):\n",
        "    scale = {'A': 0.62, 'C': 0.29, 'D': -0.90, 'E': -0.74, 'F': 1.19, 'G': 0.48, 'H': -0.40, 'I': 1.38, 'K': -1.50, 'L': 1.06, 'M': 0.64, 'N': -0.78, 'P': 0.12, 'Q': -0.85, 'R': -2.53, 'S': -0.18, 'T': -0.05, 'V': 1.08, 'W': 0.81, 'Y': 0.26}\n",
        "    return scale.get(aa, 0.0)\n",
        "\n",
        "def rose_scale(aa):\n",
        "    scale = {'A': 0.74, 'C': 0.91, 'D': 0.62, 'E': 0.62, 'F': 0.88, 'G': 0.72, 'H': 0.78, 'I': 0.88, 'K': 0.52, 'L': 0.85, 'M': 0.85, 'N': 0.63, 'P': 0.64, 'Q': 0.62, 'R': 0.64, 'S': 0.66, 'T': 0.70, 'V': 0.86, 'W': 0.85, 'Y': 0.76}\n",
        "    return scale.get(aa, 0.0)\n",
        "\n",
        "def janin_scale(aa):\n",
        "    scale = {'A': 0.30, 'C': 0.90, 'D': -0.60, 'E': -0.70, 'F': 0.50, 'G': 0.30, 'H': -0.10, 'I': 0.70, 'K': -1.80, 'L': 0.50, 'M': 0.40, 'N': -0.50, 'P': -0.30, 'Q': -0.70, 'R': -1.40, 'S': -0.10, 'T': -0.20, 'V': 0.60, 'W': 0.30, 'Y': -0.40}\n",
        "    return scale.get(aa, 0.0)\n",
        "\n",
        "def engelman_ges_scale(aa):\n",
        "    scale = {'A': 1.60, 'C': 2.00, 'D': -9.20, 'E': -8.20, 'F': 3.70, 'G': 1.00, 'H': -3.00, 'I': 3.10, 'K': -8.80, 'L': 2.80, 'M': 3.40, 'N': -4.80, 'P': -0.20, 'Q': -4.10, 'R': -12.3, 'S': 0.60, 'T': 1.20, 'V': 2.60, 'W': 1.90, 'Y': -0.70}\n",
        "    return scale.get(aa, 0.0)\n",
        "\n",
        "# Register UDFs\n",
        "kyte_doolittle_udf = udf(kyte_doolittle_scale, DoubleType())\n",
        "hopp_woods_udf = udf(hopp_woods_scale, DoubleType())\n",
        "cornette_udf = udf(cornette_scale, DoubleType())\n",
        "eisenberg_udf = udf(eisenberg_scale, DoubleType())\n",
        "rose_udf = udf(rose_scale, DoubleType())\n",
        "janin_udf = udf(janin_scale, DoubleType())\n",
        "engelman_ges_udf = udf(engelman_ges_scale, DoubleType())\n",
        "\n",
        "# Add hydrophobicity scale columns\n",
        "df2_merge = df2_merge \\\n",
        "    .withColumn('Kyte-Doolittle', kyte_doolittle_udf(col('amino'))) \\\n",
        "    .withColumn('Hopp-Woods', hopp_woods_udf(col('amino'))) \\\n",
        "    .withColumn('Cornette', cornette_udf(col('amino'))) \\\n",
        "    .withColumn('Eisenberg', eisenberg_udf(col('amino'))) \\\n",
        "    .withColumn('Rose', rose_udf(col('amino'))) \\\n",
        "    .withColumn('Janin', janin_udf(col('amino'))) \\\n",
        "    .withColumn('Engelman GES', engelman_ges_udf(col('amino')))\n",
        "\n",
        "df2_merge.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate z-score for Position\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Calculate mean and std for Position\n",
        "pos_stats = df2_merge.agg(\n",
        "    avg('Position').alias('mean_pos'),\n",
        "    stddev('Position').alias('std_pos')\n",
        ").collect()[0]\n",
        "\n",
        "mean_pos = pos_stats['mean_pos']\n",
        "std_pos = pos_stats['std_pos']\n",
        "\n",
        "# Add z-score column\n",
        "df2_merge = df2_merge.withColumn(\n",
        "    'Position z-score',\n",
        "    (col('Position') - lit(mean_pos)) / lit(std_pos)\n",
        ")\n",
        "\n",
        "print(f\"Position mean: {mean_pos:.2f}, std: {std_pos:.2f}\")\n",
        "df2_merge.select('amino', 'Position', 'Position z-score', 'Kyte-Doolittle').show(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save final dataset\n",
        "df2_final = df2_merge.select(\n",
        "    'amino', 'Position', 'label',\n",
        "    'Kyte-Doolittle', 'Hopp-Woods', 'Cornette', 'Eisenberg', \n",
        "    'Rose', 'Janin', 'Engelman GES', 'Position z-score'\n",
        ")\n",
        "\n",
        "df2_final_pd = df2_final.toPandas()\n",
        "df2_final_pd.to_csv('dataset_type_2_vers2_hidropobicity.csv', index=False)\n",
        "print(f\"Saved dataset_type_2_vers2_hidropobicity.csv with {len(df2_final_pd)} rows\")\n",
        "\n",
        "# Also save basic version\n",
        "df2_basic = df2_merge.select('amino', 'Position', 'label')\n",
        "df2_basic_pd = df2_basic.toPandas()\n",
        "df2_basic_pd.to_csv('dataset_type_2.csv', index=False)\n",
        "print(f\"Saved dataset_type_2.csv with {len(df2_basic_pd)} rows\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics\n",
        "print(\"Dataset Summary:\")\n",
        "print(f\"Total rows: {df2_final.count()}\")\n",
        "print(f\"Epitope labels: {df2_final.filter(col('label') == 'E').count()}\")\n",
        "print(f\"Non-epitope labels: {df2_final.filter(col('label') == '.').count()}\")\n",
        "\n",
        "print(\"\\nColumn Statistics:\")\n",
        "df2_final.describe().show()\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()\n",
        "print(\"\\nSpark session stopped. Processing complete!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
